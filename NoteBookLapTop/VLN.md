# Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation

论文链接

```
https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Think_Global_Act_Local_Dual-Scale_Graph_Transformer_for_Vision-and-Language_Navigation_CVPR_2022_paper.pdf
```

项目主页

```
https://cshizhe.github.io/projects/vln_duet.html
```

代码链接

```
https://github.com/cshizhe/VLN-DUET
```

# NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation

导读：本文是对于发表在 RSS 2024 的论文 *NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation* 的解读。该工作由北京大学王鹤团队与北京智源人工智能研究院、自动化研究所、阿德莱德大学、澳大利亚国立大学和北京银河通用机器人合作完成。共同第一作者包括北京大学计算机学院博士生张嘉曌，以及北京智源人工智能研究院的实习生王堃宇和许镕涛。

项目主页

```
https://pku-epic.github.io/NaVid/
```

论文

```
https://arxiv.org/pdf/2402.15852
```

# 具身导航场景自适应新范式！GSA-VLN：适应通用场景的视觉语言导航

项目地址

```
https://link.zhihu.com/?target=https%3A//github.com/honghd16/GSA-VLN
```

论文

```
https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2501.17403
```

